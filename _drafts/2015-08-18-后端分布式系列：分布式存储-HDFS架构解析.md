---
layout    : post
title     : 后端分布式系列：分布式存储-HDFS架构解析
date      : 2015-08-18
author    : mindwind
categories: blog
tags      : 分布式存储 HDFS
image     : /assets/article_images/2015-08-18.jpg
---


本文以 Hadoop 提供得分布式文件系统（HDFS）为例来进一步展开解析分布式存储服务架构设计的要点。


## 架构假设与目标
任何一种软件框架或服务都是为了解决特定问题而产生的。
还记得我们在[分布式存储－概述]({% post_url 2015-08-17-后端分布式系列：分布式存储-概述 %})一文中描述的几个关注方面么？
分布式文件系统属于分布式存储中的一种面向文件的数据模型，它需要解决单机文件系统面临的容量扩展和容错问题。

所以 HDFS 的架构设计目标就呼之欲出了：

  1. 面向超大文件或大量的文件数据集  
  2. 自动检测局部的硬件错误并快速恢复

基于此目标，考虑应用场景出于简化设计和实现的目的，HDFS 假设了一种 write-once-read-many 的文件访问模型。
这种一次写入并被大量读出的模型在现实中确实适应很多业务场景，架构设计的此类假设是合理的。
正因为此类假设的存在，也限定了它的应用场景。


## 架构设计要点
下面是一张来自官方文档的架构图：  
![](/assets/article_images/2015-08-18-1.png)

从图中可见 HDFS 的架构包括三个部分，每个部分有各自清晰的指责划分。

  1. Namenode  
  2. Datanode  
  3. Client

从图中可见，HDFS 采用的是中心总控式架构，Namenode 就是集群的中心节点。

### Namenode
Namenode 的职责是管理整个文件系统的元信息（Metadata），元信息主要包括：

  - File system namesapce  
    `HDFS 类似单机文件系统以目录树的形式组织文件，称为 file system namespace`
  - Replication factor  
    `文件副本数，针对每个文件设置`
  - Mapping of blocks to Datanodes  
    `文件块到数据节点的映射关系`

在上面架构图中，指向 Namenode 的 Metadata ops 主要就是针对文件的创建、删除、读取和设置文件的副本数等操作，
所以所有的文件操作都绕不过 Namenode。

Namenode 将所有元信息以特定的数据结构组织存放在内存中，对于 namespace 和 replication factor 的信息会进行持久化，
而映射关系则不会持久化。因为映射关系是通过 Datanode 启动后定时汇报上来，
即使 Namenode 重启后内存信息丢失也可以通过 Datanode 重新汇报获得，而其他元信息则必须通过读取持久化存储来重建内存数据结构。
出于性能原因，所有元信息的读取直接从内存中获得，而增删改操作则借鉴来数据库的事务日志技术。
每次只变更内存数据结构并记录操作日志，将随机写变为顺序写来提高吞吐能力。

Namenode 使用一个事务日志文件 EditLog 来持久化记录针对文件系统元数据的每一次操作变更。
而整个 file system namesapce 和文件、文件快和数据节点的映射关系被存储在另一个 FsImage 文件中。
当 Namenode 启动时，它读取 FsImage 文件构建元数据的内存数据结构，接着读取 EditLog 文件应用所有的操作事务到内存数据结构中。
接着基于最新的内存数据结构重新写一个最新的 FsImage 文件到磁盘上，然后清除 EditLog 的内容，
因为它上面记录所有操作日志都已经反应到 FsImage 上了。
这个过程称为建立一个检查点(checkpoint)，当然检查点技术也是数据库里最常用的了。
启动过程中 Namenode 会进入一种特殊状态，称为安全模式（Safemode state），
它等待 Datanode 报告文件块及其副本的数量并确定哪些文件的副本数量是不足的。

上面描述了 Namenode 的作用，接下来我们来看下它在架构设计上的权衡考量。
首先 Namenode 作为中心节点简化了整体设计，但很显然它也是个单点，怎么解决单点问题这里先不展开。
其次 Namenode 将所有元数据存储在内存中，内存的容量决定了整个分布式文件系统能支持文件数量，如果是大量的小文件场景也是个问题。
而且 HDFS 基于 java 实现，java 针对大堆内存的 GC 优化也是个麻烦事。
再次上面描述的 Namenode 的启动过程看起来就很耗时，特别是在 FsImage 和 EditLog 都很大的情况下。
而且在 Namenode 的启动完成前整个 HDFS 是不可用的，所以 Namenode 即使是重启也对整体的可用性有很大影响。

除了 Namenode 的单点和重启过程影响可用性外，另一个担忧因素是性能。
读操作基于内存访问还好，写操作中磁盘是一个瓶颈点。
而 Namenode 支持大量 Client 的并发读写，对于大量的并发写操作 Namenode 进行了优化。
多线程情况下，当一个线程为保存操作事务日志发起一个 flush-and-sync 到磁盘文件的操作，其他线程只能等待。
为了优化此类情况，Namenode 将随机写转换为批量写操作。
当一个 Namenode 的线程初始化了一个 flush-and-sync 操作，所有当时的事务操作日志被批量写入文件。
其余的线程只需要检查它们的事务是否被保存到了文件而不再需要再发起 flush-and-sync 操作。


## 参考
[1] Hadoop Doc. [HDFS Architecture](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html).
[2] Robert Chansler, Hairong Kuang, Sanjay Radia, Konstantin Shvachko, and Suresh Srinivas. [The Hadoop Distributed File System](http://www.aosabook.org/en/hdfs.html)
